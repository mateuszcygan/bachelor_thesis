{\rtf1\ansi\ansicpg1252\cocoartf2757
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica-Bold;\f1\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\paperw11900\paperh16840\margl1440\margr1440\vieww10080\viewh12440\viewkind0
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f0\b\fs24 \cf0 MDP:
\f1\b0 \
- from each state we can fire each action and after each action we have some outcomes with probabilities!\
\
=> for each action we should define certain following state and probability for it\
\
states\
\
\
\{ 's1' : \{ 'action': 'a' : \
		\{ 'foll_state' : 's1', 'reward' : 4, 'probability' : 0.75 \},\
		\{ 'foll_state' : 's1', 'reward' : 3, 'probability' : 0.2 \},\
		\{ 'foll_state' : 's1', 'reward' : -2, 'probability' : 0.05 \}\
	  \},\
	  \{ 'action': 'b' : \
		\{ 'foll_state' : 's3', 'reward' : 2, 'probability' : 1.0 \}\
	  \},\
	  \{ 'action': 'c' : \{ 'foll_state' : 's7', 'reward' : 9, 'probability' : 0.05\}\}\}\
\

\f0\b policy in MDP 
\f1\b0 - agent's strategy\
\
Robot moves across the room and the task is to get to target point (x, y), where it gets a reward\
A room is an environment\
Robot's current position is a state\
A policy is what an agent does to accomplish this task:\
\
dumb robots just wander around randomly until they accidentally end up in the right place (policy #1)\
others may, for some reason, learn to go along the walls most of the route (policy #2)\
smart robots plan the route in their "head" and go straight to the goal (policy #3)\
\
Reinforcement learning course David Silver: \ul https://www.youtube.com/results?search_query=david+silver+reinforcement+learning}